{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced PII Detection Agent Demo (Standalone)\n",
    "## Comprehensive PII Detection using NER, Proximity Analysis, and Graph Theory\n",
    "\n",
    "This notebook demonstrates the advanced PII detection agent that combines:\n",
    "- **Named Entity Recognition (NER)** using spaCy\n",
    "- **Proximity Analysis** for contextual risk assessment\n",
    "- **Graph Theory** for relationship mapping\n",
    "- **Risk Assessment** with multi-tier classification\n",
    "\n",
    "All code is included directly in this notebook - no external imports needed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Core Dependencies\n",
    "\n",
    "First, let's install and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install spacy pandas numpy networkx matplotlib plotly python-dotenv tqdm -q\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Data Structures and Enums\n",
    "\n",
    "Define the core data structures for PII detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIIType(Enum):\n",
    "    \"\"\"Enhanced PII classification with severity levels\"\"\"\n",
    "    # Critical PII - high re-identification risk\n",
    "    SSN = \"social_security_number\"\n",
    "    CREDIT_CARD = \"credit_card\"\n",
    "    BANK_ACCOUNT = \"bank_account\"\n",
    "    DRIVERS_LICENSE = \"drivers_license\"\n",
    "    PASSPORT = \"passport\"\n",
    "    \n",
    "    # Personal Identifiers\n",
    "    PERSON = \"person_name\"\n",
    "    EMAIL = \"email_address\"\n",
    "    PHONE = \"phone_number\"\n",
    "    DATE_OF_BIRTH = \"date_of_birth\"\n",
    "    \n",
    "    # Location Data\n",
    "    ADDRESS = \"physical_address\"\n",
    "    ZIP_CODE = \"zip_code\"\n",
    "    LOCATION = \"location\"\n",
    "    GPS_COORD = \"gps_coordinates\"\n",
    "    \n",
    "    # Organizational\n",
    "    ORGANIZATION = \"organization\"\n",
    "    WEBSITE = \"website_url\"\n",
    "    \n",
    "    # Technical\n",
    "    IP_ADDRESS = \"ip_address\"\n",
    "    MAC_ADDRESS = \"mac_address\"\n",
    "    API_KEY = \"api_key\"\n",
    "    PASSWORD = \"password\"\n",
    "    \n",
    "    # Financial\n",
    "    IBAN = \"iban\"\n",
    "    BITCOIN_ADDRESS = \"bitcoin_address\"\n",
    "    \n",
    "    # Medical\n",
    "    MEDICAL_ID = \"medical_identifier\"\n",
    "    \n",
    "    # Other\n",
    "    USERNAME = \"username\"\n",
    "    OTHER = \"other_pii\"\n",
    "\n",
    "class RiskLevel(Enum):\n",
    "    \"\"\"Risk assessment levels for PII exposure\"\"\"\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "@dataclass\n",
    "class PIIEntity:\n",
    "    \"\"\"Structured representation of a detected PII entity\"\"\"\n",
    "    text: str\n",
    "    pii_type: PIIType\n",
    "    start_pos: int\n",
    "    end_pos: int\n",
    "    confidence: float\n",
    "    context: str\n",
    "    row_index: Optional[int] = None\n",
    "    column_name: Optional[str] = None\n",
    "    risk_level: RiskLevel = RiskLevel.LOW\n",
    "    detection_method: str = \"regex\"  # \"regex\", \"ner\", \"proximity\", \"graph\"\n",
    "    related_entities: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.related_entities is None:\n",
    "            self.related_entities = []\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        result = asdict(self)\n",
    "        result['pii_type'] = self.pii_type.value\n",
    "        result['risk_level'] = self.risk_level.value\n",
    "        return result\n",
    "\n",
    "print(\"✅ Data structures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PII Detection Patterns\n",
    "\n",
    "Define regex patterns for various PII types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIIPatterns:\n",
    "    \"\"\"Compiled regex patterns for PII detection with security rationale\"\"\"\n",
    "    \n",
    "    EMAIL = re.compile(\n",
    "        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    PHONE_US = re.compile(\n",
    "        r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    )\n",
    "    \n",
    "    SSN = re.compile(\n",
    "        r'\\b\\d{3}-\\d{2}-\\d{4}\\b|\\b\\d{9}\\b'\n",
    "    )\n",
    "    \n",
    "    CREDIT_CARD = re.compile(\n",
    "        r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b'\n",
    "    )\n",
    "    \n",
    "    IP_ADDRESS = re.compile(\n",
    "        r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n",
    "    )\n",
    "    \n",
    "    API_KEY = re.compile(\n",
    "        r'\\b(?:api[_-]?key|apikey|access[_-]?token)[\\s:=]+[\\w-]{20,}\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    PASSWORD = re.compile(\n",
    "        r'\\b(?:password|passwd|pwd)[\\s:=]+\\S+\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    ZIP_CODE = re.compile(\n",
    "        r'\\b\\d{5}(?:-\\d{4})?\\b'\n",
    "    )\n",
    "    \n",
    "    DATE_OF_BIRTH = re.compile(\n",
    "        r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b'\n",
    "    )\n",
    "\n",
    "print(\"✅ PII patterns compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NER-based PII Detector\n",
    "\n",
    "Implementation of Named Entity Recognition based PII detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIINERDetector:\n",
    "    \"\"\"\n",
    "    Named Entity Recognition-based PII detector using spaCy\n",
    "    Handles person names, organizations, locations, and custom patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
    "        \"\"\"\n",
    "        Initialize NER detector with spaCy model\n",
    "        \n",
    "        Args:\n",
    "            model_name: spaCy model to use for NER\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.nlp = None\n",
    "        self._load_model()\n",
    "        self._setup_patterns()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load spaCy model with error handling\"\"\"\n",
    "        try:\n",
    "            import spacy\n",
    "            self.nlp = spacy.load(self.model_name)\n",
    "            logger.info(f\"Loaded spaCy model: {self.model_name}\")\n",
    "        except ImportError:\n",
    "            logger.error(\"spaCy not installed. Run: pip install spacy\")\n",
    "            raise ImportError(\"spaCy is required for NER functionality\")\n",
    "        except OSError:\n",
    "            logger.error(f\"spaCy model '{self.model_name}' not found. Run: python -m spacy download {self.model_name}\")\n",
    "            raise OSError(f\"spaCy model '{self.model_name}' not available\")\n",
    "    \n",
    "    def _setup_patterns(self):\n",
    "        \"\"\"Setup regex patterns for non-NER PII detection\"\"\"\n",
    "        self.patterns = {\n",
    "            PIIType.EMAIL: PIIPatterns.EMAIL,\n",
    "            PIIType.PHONE: PIIPatterns.PHONE_US,\n",
    "            PIIType.SSN: PIIPatterns.SSN,\n",
    "            PIIType.CREDIT_CARD: PIIPatterns.CREDIT_CARD,\n",
    "            PIIType.ZIP_CODE: PIIPatterns.ZIP_CODE,\n",
    "            PIIType.IP_ADDRESS: PIIPatterns.IP_ADDRESS,\n",
    "            PIIType.DATE_OF_BIRTH: PIIPatterns.DATE_OF_BIRTH,\n",
    "            PIIType.API_KEY: PIIPatterns.API_KEY,\n",
    "            PIIType.PASSWORD: PIIPatterns.PASSWORD,\n",
    "        }\n",
    "    \n",
    "    def detect_pii_in_text(self, text: str, context: Dict = None) -> List[PIIEntity]:\n",
    "        \"\"\"\n",
    "        Detect PII entities in text using both NER and regex patterns\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "            context: Additional context (row_index, column_name, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            List of detected PII entities\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        context = context or {}\n",
    "        \n",
    "        if not text or not isinstance(text, str):\n",
    "            return entities\n",
    "        \n",
    "        # Regex-based detection\n",
    "        entities.extend(self._detect_regex_pii(text, context))\n",
    "        \n",
    "        # NER-based detection\n",
    "        if self.nlp:\n",
    "            entities.extend(self._detect_ner_pii(text, context))\n",
    "        \n",
    "        # Remove duplicates and overlaps\n",
    "        entities = self._deduplicate_entities(entities)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _detect_regex_pii(self, text: str, context: Dict) -> List[PIIEntity]:\n",
    "        \"\"\"Detect PII using regex patterns\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        for pii_type, pattern in self.patterns.items():\n",
    "            matches = pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                entity = PIIEntity(\n",
    "                    text=match.group(),\n",
    "                    pii_type=pii_type,\n",
    "                    start_pos=match.start(),\n",
    "                    end_pos=match.end(),\n",
    "                    confidence=0.9,  # High confidence for regex matches\n",
    "                    context=self._extract_context(text, match.start(), match.end()),\n",
    "                    row_index=context.get('row_index'),\n",
    "                    column_name=context.get('column_name'),\n",
    "                    detection_method=\"regex\"\n",
    "                )\n",
    "                entities.append(entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _detect_ner_pii(self, text: str, context: Dict) -> List[PIIEntity]:\n",
    "        \"\"\"Detect PII using spaCy NER\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        try:\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                pii_type = self._map_ner_label_to_pii_type(ent.label_)\n",
    "                if pii_type:\n",
    "                    entity = PIIEntity(\n",
    "                        text=ent.text,\n",
    "                        pii_type=pii_type,\n",
    "                        start_pos=ent.start_char,\n",
    "                        end_pos=ent.end_char,\n",
    "                        confidence=0.8,  # NER confidence can vary\n",
    "                        context=self._extract_context(text, ent.start_char, ent.end_char),\n",
    "                        row_index=context.get('row_index'),\n",
    "                        column_name=context.get('column_name'),\n",
    "                        detection_method=\"ner\"\n",
    "                    )\n",
    "                    entities.append(entity)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"NER processing error: {e}\")\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _map_ner_label_to_pii_type(self, label: str) -> Optional[PIIType]:\n",
    "        \"\"\"Map spaCy NER labels to PII types\"\"\"\n",
    "        label_mapping = {\n",
    "            \"PERSON\": PIIType.PERSON,\n",
    "            \"ORG\": PIIType.ORGANIZATION,\n",
    "            \"GPE\": PIIType.LOCATION,  # Geopolitical entity\n",
    "            \"LOC\": PIIType.LOCATION,\n",
    "            \"FAC\": PIIType.LOCATION,  # Facility\n",
    "            \"DATE\": None,  # Generally not PII unless specific format\n",
    "            \"TIME\": None,\n",
    "            \"MONEY\": None,\n",
    "            \"PERCENT\": None,\n",
    "            \"ORDINAL\": None,\n",
    "            \"CARDINAL\": None,\n",
    "        }\n",
    "        return label_mapping.get(label)\n",
    "    \n",
    "    def _extract_context(self, text: str, start: int, end: int, window: int = 50) -> str:\n",
    "        \"\"\"Extract context around detected entity\"\"\"\n",
    "        context_start = max(0, start - window)\n",
    "        context_end = min(len(text), end + window)\n",
    "        return text[context_start:context_end]\n",
    "    \n",
    "    def _deduplicate_entities(self, entities: List[PIIEntity]) -> List[PIIEntity]:\n",
    "        \"\"\"Remove overlapping and duplicate entities, keeping highest confidence\"\"\"\n",
    "        if not entities:\n",
    "            return entities\n",
    "        \n",
    "        # Sort by start position\n",
    "        entities.sort(key=lambda x: x.start_pos)\n",
    "        \n",
    "        deduplicated = []\n",
    "        for entity in entities:\n",
    "            # Check for overlap with existing entities\n",
    "            overlaps = False\n",
    "            for existing in deduplicated:\n",
    "                if (entity.start_pos < existing.end_pos and \n",
    "                    entity.end_pos > existing.start_pos):\n",
    "                    # Overlapping entities - keep the one with higher confidence\n",
    "                    if entity.confidence > existing.confidence:\n",
    "                        deduplicated.remove(existing)\n",
    "                        deduplicated.append(entity)\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            \n",
    "            if not overlaps:\n",
    "                deduplicated.append(entity)\n",
    "        \n",
    "        return sorted(deduplicated, key=lambda x: x.start_pos)\n",
    "\n",
    "print(\"✅ NER Detector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Proximity Analyzer\n",
    "\n",
    "Analyze proximity relationships between PII entities to assess risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProximityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes proximity relationships between entities to identify contextual PII risks\n",
    "    Uses sliding window approach to find related entities that increase re-identification risk\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 100, risk_threshold: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize proximity analyzer\n",
    "        \n",
    "        Args:\n",
    "            window_size: Character window size for proximity analysis\n",
    "            risk_threshold: Threshold for high-risk proximity relationships\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.risk_threshold = risk_threshold\n",
    "        \n",
    "        # Define risk relationships between PII types\n",
    "        self.risk_matrix = self._build_risk_matrix()\n",
    "    \n",
    "    def _build_risk_matrix(self) -> Dict[Tuple[PIIType, PIIType], float]:\n",
    "        \"\"\"Build matrix of risk scores for PII type combinations\"\"\"\n",
    "        risk_matrix = {}\n",
    "        \n",
    "        # High-risk combinations (score >= 0.8)\n",
    "        high_risk_pairs = [\n",
    "            (PIIType.PERSON, PIIType.SSN),\n",
    "            (PIIType.PERSON, PIIType.DATE_OF_BIRTH),\n",
    "            (PIIType.PERSON, PIIType.ADDRESS),\n",
    "            (PIIType.EMAIL, PIIType.PHONE),\n",
    "            (PIIType.PERSON, PIIType.PHONE),\n",
    "            (PIIType.ORGANIZATION, PIIType.EMAIL),\n",
    "            (PIIType.ZIP_CODE, PIIType.ADDRESS),\n",
    "            (PIIType.PERSON, PIIType.CREDIT_CARD),\n",
    "        ]\n",
    "        \n",
    "        # Medium-risk combinations (score >= 0.5)\n",
    "        medium_risk_pairs = [\n",
    "            (PIIType.PERSON, PIIType.ORGANIZATION),\n",
    "            (PIIType.EMAIL, PIIType.ORGANIZATION),\n",
    "            (PIIType.PHONE, PIIType.ADDRESS),\n",
    "            (PIIType.LOCATION, PIIType.ZIP_CODE),\n",
    "            (PIIType.PERSON, PIIType.EMAIL),\n",
    "            (PIIType.PERSON, PIIType.LOCATION),\n",
    "        ]\n",
    "        \n",
    "        # Assign risk scores\n",
    "        for pair in high_risk_pairs:\n",
    "            risk_matrix[pair] = 0.9\n",
    "            risk_matrix[(pair[1], pair[0])] = 0.9  # Symmetric\n",
    "        \n",
    "        for pair in medium_risk_pairs:\n",
    "            risk_matrix[pair] = 0.6\n",
    "            risk_matrix[(pair[1], pair[0])] = 0.6  # Symmetric\n",
    "        \n",
    "        return risk_matrix\n",
    "    \n",
    "    def analyze_proximity(self, entities: List[PIIEntity], text: str) -> List[PIIEntity]:\n",
    "        \"\"\"\n",
    "        Analyze proximity relationships and update entity risk levels\n",
    "        \n",
    "        Args:\n",
    "            entities: List of detected PII entities\n",
    "            text: Original text for context analysis\n",
    "            \n",
    "        Returns:\n",
    "            Updated entities with proximity-based risk assessments\n",
    "        \"\"\"\n",
    "        if len(entities) < 2:\n",
    "            return entities\n",
    "        \n",
    "        # Create proximity groups\n",
    "        proximity_groups = self._create_proximity_groups(entities)\n",
    "        \n",
    "        # Analyze each group for risk relationships\n",
    "        for group in proximity_groups:\n",
    "            self._analyze_group_risk(group, text)\n",
    "        \n",
    "        # Update entity risk levels based on proximity analysis\n",
    "        updated_entities = []\n",
    "        for entity in entities:\n",
    "            updated_entity = self._update_entity_risk(entity, proximity_groups)\n",
    "            updated_entities.append(updated_entity)\n",
    "        \n",
    "        return updated_entities\n",
    "    \n",
    "    def _create_proximity_groups(self, entities: List[PIIEntity]) -> List[List[PIIEntity]]:\n",
    "        \"\"\"Group entities that are within proximity window of each other\"\"\"\n",
    "        groups = []\n",
    "        used_entities = set()\n",
    "        \n",
    "        for i, entity in enumerate(entities):\n",
    "            if i in used_entities:\n",
    "                continue\n",
    "            \n",
    "            group = [entity]\n",
    "            used_entities.add(i)\n",
    "            \n",
    "            # Find all entities within proximity window\n",
    "            for j, other_entity in enumerate(entities[i+1:], i+1):\n",
    "                if j in used_entities:\n",
    "                    continue\n",
    "                \n",
    "                if self._are_proximate(entity, other_entity):\n",
    "                    group.append(other_entity)\n",
    "                    used_entities.add(j)\n",
    "            \n",
    "            if len(group) > 1:  # Only include groups with multiple entities\n",
    "                groups.append(group)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def _are_proximate(self, entity1: PIIEntity, entity2: PIIEntity) -> bool:\n",
    "        \"\"\"Check if two entities are within proximity window\"\"\"\n",
    "        return abs(entity1.start_pos - entity2.start_pos) <= self.window_size\n",
    "    \n",
    "    def _analyze_group_risk(self, group: List[PIIEntity], text: str):\n",
    "        \"\"\"Analyze risk level for a group of proximate entities\"\"\"\n",
    "        group_types = [entity.pii_type for entity in group]\n",
    "        \n",
    "        # Calculate maximum risk score for the group\n",
    "        max_risk = 0.0\n",
    "        risk_pairs = []\n",
    "        \n",
    "        for i, entity1 in enumerate(group):\n",
    "            for entity2 in group[i+1:]:\n",
    "                pair_key = (entity1.pii_type, entity2.pii_type)\n",
    "                risk_score = self.risk_matrix.get(pair_key, 0.3)  # Default low risk\n",
    "                \n",
    "                if risk_score > max_risk:\n",
    "                    max_risk = risk_score\n",
    "                \n",
    "                if risk_score >= 0.5:  # Medium or higher risk\n",
    "                    risk_pairs.append((entity1, entity2, risk_score))\n",
    "        \n",
    "        # Update related entities information\n",
    "        for entity in group:\n",
    "            entity.related_entities = [\n",
    "                f\"{other.pii_type.value}:{other.text[:20]}...\" \n",
    "                for other in group if other != entity\n",
    "            ]\n",
    "    \n",
    "    def _update_entity_risk(self, entity: PIIEntity, proximity_groups: List[List[PIIEntity]]) -> PIIEntity:\n",
    "        \"\"\"Update entity risk level based on proximity analysis\"\"\"\n",
    "        # Find which group this entity belongs to\n",
    "        entity_group = None\n",
    "        for group in proximity_groups:\n",
    "            if entity in group:\n",
    "                entity_group = group\n",
    "                break\n",
    "        \n",
    "        if not entity_group or len(entity_group) == 1:\n",
    "            # No proximity relationships\n",
    "            entity.risk_level = self._get_base_risk_level(entity.pii_type)\n",
    "            return entity\n",
    "        \n",
    "        # Calculate risk based on proximity relationships\n",
    "        max_proximity_risk = 0.0\n",
    "        for other_entity in entity_group:\n",
    "            if other_entity != entity:\n",
    "                pair_key = (entity.pii_type, other_entity.pii_type)\n",
    "                risk = self.risk_matrix.get(pair_key, 0.3)\n",
    "                max_proximity_risk = max(max_proximity_risk, risk)\n",
    "        \n",
    "        # Combine base risk with proximity risk\n",
    "        base_risk = self._get_base_risk_score(entity.pii_type)\n",
    "        combined_risk = min(1.0, base_risk + max_proximity_risk * 0.5)\n",
    "        \n",
    "        entity.risk_level = self._score_to_risk_level(combined_risk)\n",
    "        entity.detection_method = \"proximity\"\n",
    "        \n",
    "        return entity\n",
    "    \n",
    "    def _get_base_risk_level(self, pii_type: PIIType) -> RiskLevel:\n",
    "        \"\"\"Get base risk level for a PII type\"\"\"\n",
    "        critical_types = {PIIType.SSN, PIIType.CREDIT_CARD, PIIType.PASSPORT, \n",
    "                         PIIType.DRIVERS_LICENSE, PIIType.BANK_ACCOUNT}\n",
    "        high_types = {PIIType.DATE_OF_BIRTH, PIIType.ADDRESS, PIIType.MEDICAL_ID}\n",
    "        medium_types = {PIIType.PERSON, PIIType.EMAIL, PIIType.PHONE}\n",
    "        \n",
    "        if pii_type in critical_types:\n",
    "            return RiskLevel.CRITICAL\n",
    "        elif pii_type in high_types:\n",
    "            return RiskLevel.HIGH\n",
    "        elif pii_type in medium_types:\n",
    "            return RiskLevel.MEDIUM\n",
    "        else:\n",
    "            return RiskLevel.LOW\n",
    "    \n",
    "    def _get_base_risk_score(self, pii_type: PIIType) -> float:\n",
    "        \"\"\"Get base risk score for a PII type\"\"\"\n",
    "        risk_level = self._get_base_risk_level(pii_type)\n",
    "        score_mapping = {\n",
    "            RiskLevel.CRITICAL: 0.9,\n",
    "            RiskLevel.HIGH: 0.7,\n",
    "            RiskLevel.MEDIUM: 0.5,\n",
    "            RiskLevel.LOW: 0.3\n",
    "        }\n",
    "        return score_mapping[risk_level]\n",
    "    \n",
    "    def _score_to_risk_level(self, score: float) -> RiskLevel:\n",
    "        \"\"\"Convert risk score to risk level\"\"\"\n",
    "        if score >= 0.85:\n",
    "            return RiskLevel.CRITICAL\n",
    "        elif score >= 0.65:\n",
    "            return RiskLevel.HIGH\n",
    "        elif score >= 0.45:\n",
    "            return RiskLevel.MEDIUM\n",
    "        else:\n",
    "            return RiskLevel.LOW\n",
    "\n",
    "print(\"✅ Proximity Analyzer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph Builder for Entity Relationships\n",
    "\n",
    "Build and analyze entity relationship graphs using NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIIGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds and analyzes entity graphs using networkx to identify PII clusters\n",
    "    and relationships that may increase re-identification risks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_edge_weight: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize graph builder\n",
    "        \n",
    "        Args:\n",
    "            min_edge_weight: Minimum weight for edges to be included in graph\n",
    "        \"\"\"\n",
    "        self.min_edge_weight = min_edge_weight\n",
    "        self.graph = nx.Graph()\n",
    "        self.entity_metadata = {}\n",
    "    \n",
    "    def build_graph(self, entities: List[PIIEntity], text: str = None) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Build entity relationship graph\n",
    "        \n",
    "        Args:\n",
    "            entities: List of PII entities\n",
    "            text: Original text for context (optional)\n",
    "            \n",
    "        Returns:\n",
    "            NetworkX graph with entities as nodes and relationships as edges\n",
    "        \"\"\"\n",
    "        self.graph.clear()\n",
    "        self.entity_metadata.clear()\n",
    "        \n",
    "        if not entities:\n",
    "            return self.graph\n",
    "        \n",
    "        # Add nodes\n",
    "        for i, entity in enumerate(entities):\n",
    "            node_id = f\"{entity.pii_type.value}_{i}\"\n",
    "            \n",
    "            self.graph.add_node(\n",
    "                node_id,\n",
    "                pii_type=entity.pii_type.value,\n",
    "                text=entity.text[:50],  # Truncate for privacy\n",
    "                confidence=entity.confidence,\n",
    "                risk_level=entity.risk_level.value,\n",
    "                row_index=entity.row_index,\n",
    "                column_name=entity.column_name,\n",
    "                detection_method=entity.detection_method\n",
    "            )\n",
    "            \n",
    "            self.entity_metadata[node_id] = entity\n",
    "        \n",
    "        # Add edges based on relationships\n",
    "        self._add_proximity_edges(entities)\n",
    "        self._add_semantic_edges(entities)\n",
    "        self._add_co_occurrence_edges(entities)\n",
    "        \n",
    "        return self.graph\n",
    "    \n",
    "    def _add_proximity_edges(self, entities: List[PIIEntity]):\n",
    "        \"\"\"Add edges between entities that are spatially close\"\"\"\n",
    "        proximity_analyzer = ProximityAnalyzer()\n",
    "        \n",
    "        for i, entity1 in enumerate(entities):\n",
    "            node1_id = f\"{entity1.pii_type.value}_{i}\"\n",
    "            \n",
    "            for j, entity2 in enumerate(entities[i+1:], i+1):\n",
    "                node2_id = f\"{entity2.pii_type.value}_{j}\"\n",
    "                \n",
    "                # Calculate proximity weight\n",
    "                distance = abs(entity1.start_pos - entity2.start_pos)\n",
    "                if distance <= proximity_analyzer.window_size:\n",
    "                    # Inverse distance weighting\n",
    "                    weight = max(0.1, 1.0 - (distance / proximity_analyzer.window_size))\n",
    "                    \n",
    "                    if weight >= self.min_edge_weight:\n",
    "                        self.graph.add_edge(\n",
    "                            node1_id, node2_id,\n",
    "                            weight=weight,\n",
    "                            edge_type=\"proximity\",\n",
    "                            distance=distance\n",
    "                        )\n",
    "    \n",
    "    def _add_semantic_edges(self, entities: List[PIIEntity]):\n",
    "        \"\"\"Add edges between semantically related entities\"\"\"\n",
    "        semantic_relationships = {\n",
    "            (PIIType.PERSON, PIIType.EMAIL): 0.8,\n",
    "            (PIIType.PERSON, PIIType.PHONE): 0.8,\n",
    "            (PIIType.PERSON, PIIType.ADDRESS): 0.9,\n",
    "            (PIIType.ORGANIZATION, PIIType.EMAIL): 0.7,\n",
    "            (PIIType.ORGANIZATION, PIIType.ADDRESS): 0.8,\n",
    "            (PIIType.ADDRESS, PIIType.ZIP_CODE): 0.9,\n",
    "            (PIIType.EMAIL, PIIType.USERNAME): 0.6,\n",
    "        }\n",
    "        \n",
    "        for i, entity1 in enumerate(entities):\n",
    "            node1_id = f\"{entity1.pii_type.value}_{i}\"\n",
    "            \n",
    "            for j, entity2 in enumerate(entities[i+1:], i+1):\n",
    "                node2_id = f\"{entity2.pii_type.value}_{j}\"\n",
    "                \n",
    "                # Check for semantic relationship\n",
    "                pair_key = (entity1.pii_type, entity2.pii_type)\n",
    "                reverse_key = (entity2.pii_type, entity1.pii_type)\n",
    "                \n",
    "                weight = semantic_relationships.get(pair_key) or semantic_relationships.get(reverse_key)\n",
    "                \n",
    "                if weight and weight >= self.min_edge_weight:\n",
    "                    self.graph.add_edge(\n",
    "                        node1_id, node2_id,\n",
    "                        weight=weight,\n",
    "                        edge_type=\"semantic\"\n",
    "                    )\n",
    "    \n",
    "    def _add_co_occurrence_edges(self, entities: List[PIIEntity]):\n",
    "        \"\"\"Add edges between entities that co-occur in the same row/column\"\"\"\n",
    "        # Group entities by row and column\n",
    "        row_groups = defaultdict(list)\n",
    "        col_groups = defaultdict(list)\n",
    "        \n",
    "        for i, entity in enumerate(entities):\n",
    "            node_id = f\"{entity.pii_type.value}_{i}\"\n",
    "            \n",
    "            if entity.row_index is not None:\n",
    "                row_groups[entity.row_index].append(node_id)\n",
    "            \n",
    "            if entity.column_name is not None:\n",
    "                col_groups[entity.column_name].append(node_id)\n",
    "        \n",
    "        # Add edges within row groups\n",
    "        for row_entities in row_groups.values():\n",
    "            if len(row_entities) > 1:\n",
    "                for i, node1 in enumerate(row_entities):\n",
    "                    for node2 in row_entities[i+1:]:\n",
    "                        if not self.graph.has_edge(node1, node2):\n",
    "                            self.graph.add_edge(\n",
    "                                node1, node2,\n",
    "                                weight=0.3,\n",
    "                                edge_type=\"row_co_occurrence\"\n",
    "                            )\n",
    "        \n",
    "        # Add edges within column groups (lighter weight)\n",
    "        for col_entities in col_groups.values():\n",
    "            if len(col_entities) > 1:\n",
    "                for i, node1 in enumerate(col_entities):\n",
    "                    for node2 in col_entities[i+1:]:\n",
    "                        if not self.graph.has_edge(node1, node2):\n",
    "                            self.graph.add_edge(\n",
    "                                node1, node2,\n",
    "                                weight=0.2,\n",
    "                                edge_type=\"column_co_occurrence\"\n",
    "                            )\n",
    "    \n",
    "    def analyze_graph(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive graph analysis\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with graph analysis results\n",
    "        \"\"\"\n",
    "        if not self.graph.nodes():\n",
    "            return {\"error\": \"Empty graph - no entities to analyze\"}\n",
    "        \n",
    "        analysis = {}\n",
    "        \n",
    "        # Basic graph metrics\n",
    "        analysis[\"basic_metrics\"] = {\n",
    "            \"num_nodes\": self.graph.number_of_nodes(),\n",
    "            \"num_edges\": self.graph.number_of_edges(),\n",
    "            \"density\": nx.density(self.graph),\n",
    "            \"is_connected\": nx.is_connected(self.graph)\n",
    "        }\n",
    "        \n",
    "        # Connected components analysis\n",
    "        components = list(nx.connected_components(self.graph))\n",
    "        analysis[\"connected_components\"] = {\n",
    "            \"count\": len(components),\n",
    "            \"sizes\": [len(comp) for comp in components],\n",
    "            \"largest_component_size\": max([len(comp) for comp in components]) if components else 0\n",
    "        }\n",
    "        \n",
    "        # Centrality measures\n",
    "        if self.graph.number_of_nodes() > 1:\n",
    "            analysis[\"centrality\"] = {\n",
    "                \"degree_centrality\": dict(nx.degree_centrality(self.graph)),\n",
    "                \"betweenness_centrality\": dict(nx.betweenness_centrality(self.graph)),\n",
    "                \"closeness_centrality\": dict(nx.closeness_centrality(self.graph))\n",
    "            }\n",
    "        \n",
    "        # Risk cluster identification\n",
    "        analysis[\"risk_clusters\"] = self._identify_risk_clusters(components)\n",
    "        \n",
    "        # Edge analysis\n",
    "        analysis[\"edge_analysis\"] = self._analyze_edges()\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _identify_risk_clusters(self, components: List[Set]) -> List[Dict]:\n",
    "        \"\"\"Identify high-risk clusters of connected entities\"\"\"\n",
    "        risk_clusters = []\n",
    "        \n",
    "        for i, component in enumerate(components):\n",
    "            if len(component) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Calculate cluster risk score\n",
    "            cluster_entities = [self.entity_metadata[node_id] for node_id in component]\n",
    "            risk_scores = [self._get_risk_score(entity.risk_level) for entity in cluster_entities]\n",
    "            avg_risk = np.mean(risk_scores)\n",
    "            max_risk = max(risk_scores)\n",
    "            \n",
    "            # Analyze PII type diversity\n",
    "            pii_types = {entity.pii_type for entity in cluster_entities}\n",
    "            type_diversity = len(pii_types) / len(cluster_entities)\n",
    "            \n",
    "            cluster_info = {\n",
    "                \"cluster_id\": i,\n",
    "                \"size\": len(component),\n",
    "                \"entities\": [\n",
    "                    {\n",
    "                        \"type\": entity.pii_type.value,\n",
    "                        \"text_preview\": entity.text[:20] + \"...\" if len(entity.text) > 20 else entity.text,\n",
    "                        \"risk_level\": entity.risk_level.value,\n",
    "                        \"confidence\": entity.confidence\n",
    "                    }\n",
    "                    for entity in cluster_entities\n",
    "                ],\n",
    "                \"average_risk_score\": avg_risk,\n",
    "                \"max_risk_score\": max_risk,\n",
    "                \"type_diversity\": type_diversity,\n",
    "                \"overall_risk\": self._calculate_cluster_risk(avg_risk, max_risk, type_diversity, len(component))\n",
    "            }\n",
    "            \n",
    "            risk_clusters.append(cluster_info)\n",
    "        \n",
    "        # Sort by overall risk\n",
    "        risk_clusters.sort(key=lambda x: x[\"overall_risk\"], reverse=True)\n",
    "        \n",
    "        return risk_clusters\n",
    "    \n",
    "    def _get_risk_score(self, risk_level: RiskLevel) -> float:\n",
    "        \"\"\"Convert risk level to numeric score\"\"\"\n",
    "        score_mapping = {\n",
    "            RiskLevel.LOW: 0.25,\n",
    "            RiskLevel.MEDIUM: 0.5,\n",
    "            RiskLevel.HIGH: 0.75,\n",
    "            RiskLevel.CRITICAL: 1.0\n",
    "        }\n",
    "        return score_mapping[risk_level]\n",
    "    \n",
    "    def _calculate_cluster_risk(self, avg_risk: float, max_risk: float, \n",
    "                               type_diversity: float, cluster_size: int) -> float:\n",
    "        \"\"\"Calculate overall cluster risk score\"\"\"\n",
    "        # Weighted combination of factors\n",
    "        size_factor = min(1.0, cluster_size / 5.0)  # Larger clusters are riskier\n",
    "        diversity_factor = type_diversity  # More diverse types increase risk\n",
    "        \n",
    "        overall_risk = (\n",
    "            0.4 * avg_risk +\n",
    "            0.3 * max_risk +\n",
    "            0.2 * diversity_factor +\n",
    "            0.1 * size_factor\n",
    "        )\n",
    "        \n",
    "        return min(1.0, overall_risk)\n",
    "    \n",
    "    def _analyze_edges(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze edge patterns and types\"\"\"\n",
    "        edge_data = []\n",
    "        \n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            edge_data.append({\n",
    "                \"source\": u,\n",
    "                \"target\": v,\n",
    "                \"weight\": data.get(\"weight\", 0),\n",
    "                \"type\": data.get(\"edge_type\", \"unknown\")\n",
    "            })\n",
    "        \n",
    "        if not edge_data:\n",
    "            return {\"total_edges\": 0}\n",
    "        \n",
    "        # Edge type distribution\n",
    "        edge_types = [edge[\"type\"] for edge in edge_data]\n",
    "        type_counts = {edge_type: edge_types.count(edge_type) for edge_type in set(edge_types)}\n",
    "        \n",
    "        # Weight distribution\n",
    "        weights = [edge[\"weight\"] for edge in edge_data]\n",
    "        \n",
    "        return {\n",
    "            \"total_edges\": len(edge_data),\n",
    "            \"edge_type_distribution\": type_counts,\n",
    "            \"weight_statistics\": {\n",
    "                \"mean\": np.mean(weights),\n",
    "                \"std\": np.std(weights),\n",
    "                \"min\": np.min(weights),\n",
    "                \"max\": np.max(weights)\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"✅ Graph Builder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Sample Data\n",
    "\n",
    "Let's create various sample datasets to demonstrate PII detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer data with various PII types\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': ['CUST001', 'CUST002', 'CUST003', 'CUST004', 'CUST005'],\n",
    "    'full_name': ['John Doe', 'Jane Smith', 'Robert Johnson', 'Maria Garcia', 'David Lee'],\n",
    "    'email': ['john.doe@example.com', 'jane.smith@company.org', 'rjohnson@email.net', \n",
    "              'maria.g@domain.com', 'dlee@business.co'],\n",
    "    'phone': ['555-123-4567', '(555) 987-6543', '+1 555-555-5555', \n",
    "              '555.444.3333', '1-555-222-1111'],\n",
    "    'ssn': ['123-45-6789', '987-65-4321', '456-78-9123', '321-54-9876', '789-12-3456'],\n",
    "    'credit_card': ['4111-1111-1111-1111', '5555-4444-3333-2222', '3782-8224-6310-005',\n",
    "                   '6011-1111-1111-1117', '4532-1234-5678-9010'],\n",
    "    'address': ['123 Main St, New York, NY 10001', '456 Oak Ave, Los Angeles, CA 90001',\n",
    "               '789 Pine Rd, Chicago, IL 60601', '321 Elm St, Houston, TX 77001',\n",
    "               '654 Maple Dr, Phoenix, AZ 85001'],\n",
    "    'notes': ['VIP customer since 2020', 'Prefers email communication', \n",
    "             'Contact after 5pm only', 'Spanish speaking preferred', \n",
    "             'Account manager: Sarah Wilson']\n",
    "})\n",
    "\n",
    "print(\"Sample customer data created:\")\n",
    "print(f\"Shape: {customer_data.shape}\")\n",
    "print(f\"Columns: {list(customer_data.columns)}\")\n",
    "display(customer_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstrate PII Detection Pipeline\n",
    "\n",
    "Now let's demonstrate the complete PII detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "ner_detector = PIINERDetector()\n",
    "proximity_analyzer = ProximityAnalyzer(window_size=50)\n",
    "graph_builder = PIIGraphBuilder()\n",
    "\n",
    "# Test on a sample text\n",
    "test_text = \"\"\"John Doe (john.doe@example.com) called from 555-123-4567 regarding \n",
    "his account. His SSN is 123-45-6789 and he wants to update his credit card \n",
    "ending in 1111. He works at Acme Corporation in New York.\"\"\"\n",
    "\n",
    "print(\"Test Text:\")\n",
    "print(test_text)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 1: Detect PII\n",
    "print(\"Step 1: PII Detection\")\n",
    "print(\"-\" * 30)\n",
    "entities = ner_detector.detect_pii_in_text(test_text)\n",
    "print(f\"Found {len(entities)} PII entities:\\n\")\n",
    "for entity in entities:\n",
    "    print(f\"  📍 {entity.pii_type.value:20} | '{entity.text}'\")\n",
    "    print(f\"     Position: [{entity.start_pos}:{entity.end_pos}] | Confidence: {entity.confidence:.2f}\")\n",
    "\n",
    "# Step 2: Proximity Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Step 2: Proximity Analysis\")\n",
    "print(\"-\" * 30)\n",
    "analyzed_entities = proximity_analyzer.analyze_proximity(entities, test_text)\n",
    "print(\"Entities with elevated risk due to proximity:\\n\")\n",
    "for entity in analyzed_entities:\n",
    "    if entity.related_entities:\n",
    "        print(f\"  🔗 {entity.pii_type.value}: '{entity.text}'\")\n",
    "        print(f\"     Risk Level: {entity.risk_level.value.upper()}\")\n",
    "        print(f\"     Related to: {len(entity.related_entities)} other entities\")\n",
    "\n",
    "# Step 3: Graph Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Step 3: Graph Analysis\")\n",
    "print(\"-\" * 30)\n",
    "graph = graph_builder.build_graph(analyzed_entities)\n",
    "analysis = graph_builder.analyze_graph()\n",
    "\n",
    "print(f\"Graph Metrics:\")\n",
    "print(f\"  • Nodes: {analysis['basic_metrics']['num_nodes']}\")\n",
    "print(f\"  • Edges: {analysis['basic_metrics']['num_edges']}\")\n",
    "print(f\"  • Density: {analysis['basic_metrics']['density']:.3f}\")\n",
    "print(f\"  • Connected Components: {analysis['connected_components']['count']}\")\n",
    "\n",
    "if analysis.get('risk_clusters'):\n",
    "    print(f\"\\nHigh-Risk Clusters:\")\n",
    "    for cluster in analysis['risk_clusters'][:2]:\n",
    "        print(f\"  • Cluster {cluster['cluster_id']}: {cluster['size']} entities\")\n",
    "        print(f\"    Overall Risk: {cluster['overall_risk']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process CSV Data\n",
    "\n",
    "Process the sample CSV data and generate masked output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Process a DataFrame for PII detection and masking\"\"\"\n",
    "    \n",
    "    detector = PIINERDetector()\n",
    "    analyzer = ProximityAnalyzer()\n",
    "    builder = PIIGraphBuilder()\n",
    "    \n",
    "    all_entities = []\n",
    "    masked_df = df.copy()\n",
    "    \n",
    "    # Process each cell\n",
    "    for row_idx, row in df.iterrows():\n",
    "        for col_name, cell_value in row.items():\n",
    "            if pd.isna(cell_value) or cell_value == '':\n",
    "                continue\n",
    "            \n",
    "            cell_str = str(cell_value)\n",
    "            context = {\n",
    "                'row_index': row_idx,\n",
    "                'column_name': col_name\n",
    "            }\n",
    "            \n",
    "            # Detect PII in cell\n",
    "            entities = detector.detect_pii_in_text(cell_str, context)\n",
    "            \n",
    "            if entities:\n",
    "                all_entities.extend(entities)\n",
    "                \n",
    "                # Mask PII in the cell\n",
    "                masked_value = cell_str\n",
    "                # Sort entities by position (reverse order for proper replacement)\n",
    "                entities.sort(key=lambda x: x.start_pos, reverse=True)\n",
    "                \n",
    "                for entity in entities:\n",
    "                    mask_token = f\"[{entity.pii_type.value.upper()}_REDACTED]\"\n",
    "                    masked_value = (\n",
    "                        masked_value[:entity.start_pos] + \n",
    "                        mask_token + \n",
    "                        masked_value[entity.end_pos:]\n",
    "                    )\n",
    "                \n",
    "                masked_df.at[row_idx, col_name] = masked_value\n",
    "    \n",
    "    # Analyze relationships\n",
    "    if all_entities:\n",
    "        all_entities = analyzer.analyze_proximity(all_entities, \"\")\n",
    "        graph = builder.build_graph(all_entities)\n",
    "        graph_analysis = builder.analyze_graph()\n",
    "    else:\n",
    "        graph_analysis = {}\n",
    "    \n",
    "    # Calculate risk distribution\n",
    "    risk_distribution = defaultdict(int)\n",
    "    for entity in all_entities:\n",
    "        risk_distribution[entity.risk_level.value] += 1\n",
    "    \n",
    "    # Get PII types detected\n",
    "    pii_types_detected = list(set(e.pii_type.value for e in all_entities))\n",
    "    \n",
    "    results = {\n",
    "        'summary': {\n",
    "            'total_entities': len(all_entities),\n",
    "            'pii_types_detected': pii_types_detected,\n",
    "            'risk_distribution': dict(risk_distribution)\n",
    "        },\n",
    "        'masked_df': masked_df,\n",
    "        'entities': all_entities,\n",
    "        'graph_analysis': graph_analysis\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process the customer data\n",
    "print(\"Processing customer data...\")\n",
    "results = process_dataframe(customer_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"  Total PII entities detected: {results['summary']['total_entities']}\")\n",
    "print(f\"  PII types found: {', '.join(results['summary']['pii_types_detected'])}\")\n",
    "print(f\"  Risk distribution: {json.dumps(results['summary']['risk_distribution'], indent=4)}\")\n",
    "\n",
    "print(\"\\n📄 Original vs Masked Data:\")\n",
    "print(\"\\nOriginal (first row):\")\n",
    "display(customer_data.head(1))\n",
    "\n",
    "print(\"\\nMasked (first row):\")\n",
    "display(results['masked_df'].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Results\n",
    "\n",
    "Create visualizations of the PII detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PII types distribution\n",
    "pii_types = [entity.pii_type.value for entity in results['entities']]\n",
    "pii_counts = pd.Series(pii_types).value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: PII Types Distribution\n",
    "ax1 = axes[0, 0]\n",
    "bars = ax1.bar(range(len(pii_counts)), pii_counts.values)\n",
    "ax1.set_xticks(range(len(pii_counts)))\n",
    "ax1.set_xticklabels(pii_counts.index, rotation=45, ha='right')\n",
    "ax1.set_xlabel('PII Type')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('PII Types Detected')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Color code by risk\n",
    "colors = []\n",
    "for pii_type in pii_counts.index:\n",
    "    if pii_type in ['social_security_number', 'credit_card']:\n",
    "        colors.append('red')\n",
    "    elif pii_type in ['email_address', 'phone_number']:\n",
    "        colors.append('orange')\n",
    "    else:\n",
    "        colors.append('yellow')\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "# Plot 2: Risk Level Distribution\n",
    "ax2 = axes[0, 1]\n",
    "risk_levels = [entity.risk_level.value for entity in results['entities']]\n",
    "risk_counts = pd.Series(risk_levels).value_counts()\n",
    "colors_risk = {'critical': '#FF0000', 'high': '#FF6600', 'medium': '#FFAA00', 'low': '#FFFF00'}\n",
    "wedges, texts, autotexts = ax2.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',\n",
    "        colors=[colors_risk.get(level, '#CCCCCC') for level in risk_counts.index],\n",
    "        startangle=90)\n",
    "ax2.set_title('Risk Level Distribution')\n",
    "\n",
    "# Plot 3: PII by Column\n",
    "ax3 = axes[1, 0]\n",
    "column_counts = defaultdict(int)\n",
    "for entity in results['entities']:\n",
    "    if entity.column_name:\n",
    "        column_counts[entity.column_name] += 1\n",
    "column_df = pd.Series(column_counts)\n",
    "ax3.bar(range(len(column_df)), column_df.values, color='steelblue')\n",
    "ax3.set_xticks(range(len(column_df)))\n",
    "ax3.set_xticklabels(column_df.index, rotation=45, ha='right')\n",
    "ax3.set_xlabel('Column')\n",
    "ax3.set_ylabel('PII Count')\n",
    "ax3.set_title('PII Distribution by Column')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Detection Method Distribution\n",
    "ax4 = axes[1, 1]\n",
    "methods = [entity.detection_method for entity in results['entities']]\n",
    "method_counts = pd.Series(methods).value_counts()\n",
    "ax4.bar(range(len(method_counts)), method_counts.values, color='coral')\n",
    "ax4.set_xticks(range(len(method_counts)))\n",
    "ax4.set_xticklabels(method_counts.index)\n",
    "ax4.set_xlabel('Detection Method')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('Detection Methods Used')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('PII Detection Analysis Dashboard', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Compliance Report\n",
    "\n",
    "Create a compliance-focused report for regulatory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_compliance_report(results: Dict) -> Dict:\n",
    "    \"\"\"Generate a compliance report based on PII detection results\"\"\"\n",
    "    \n",
    "    summary = results['summary']\n",
    "    pii_types = summary['pii_types_detected']\n",
    "    \n",
    "    # Define regulatory mappings\n",
    "    regulations = {\n",
    "        'GDPR': {\n",
    "            'affected_types': ['person_name', 'email_address', 'phone_number', 'physical_address', 'ip_address'],\n",
    "            'requirements': 'Requires explicit consent, right to erasure, data portability'\n",
    "        },\n",
    "        'HIPAA': {\n",
    "            'affected_types': ['person_name', 'social_security_number', 'medical_identifier', 'date_of_birth'],\n",
    "            'requirements': 'Requires encryption, access controls, audit logs'\n",
    "        },\n",
    "        'PCI_DSS': {\n",
    "            'affected_types': ['credit_card', 'bank_account'],\n",
    "            'requirements': 'Requires tokenization, network segmentation, regular security scans'\n",
    "        },\n",
    "        'CCPA': {\n",
    "            'affected_types': ['person_name', 'email_address', 'phone_number', 'ip_address', 'physical_address'],\n",
    "            'requirements': 'Requires opt-out mechanism, data disclosure, non-discrimination'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'total_pii_entities': summary['total_entities'],\n",
    "        'affected_regulations': [],\n",
    "        'recommendations': [],\n",
    "        'risk_score': 0\n",
    "    }\n",
    "    \n",
    "    # Check which regulations apply\n",
    "    for reg_name, reg_info in regulations.items():\n",
    "        affected_types = [pii_type for pii_type in pii_types if pii_type in reg_info['affected_types']]\n",
    "        if affected_types:\n",
    "            report['affected_regulations'].append({\n",
    "                'regulation': reg_name,\n",
    "                'affected_pii_types': affected_types,\n",
    "                'requirements': reg_info['requirements']\n",
    "            })\n",
    "    \n",
    "    # Calculate overall risk score\n",
    "    risk_dist = summary.get('risk_distribution', {})\n",
    "    risk_weights = {'critical': 1.0, 'high': 0.7, 'medium': 0.4, 'low': 0.1}\n",
    "    total_weighted = sum(risk_dist.get(level, 0) * weight \n",
    "                        for level, weight in risk_weights.items())\n",
    "    max_possible = summary['total_entities']\n",
    "    report['risk_score'] = (total_weighted / max_possible * 100) if max_possible > 0 else 0\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if report['risk_score'] > 70:\n",
    "        report['recommendations'].append(\"CRITICAL: Immediate remediation required\")\n",
    "        report['recommendations'].append(\"Implement data encryption at rest and in transit\")\n",
    "        report['recommendations'].append(\"Review and restrict data access permissions\")\n",
    "    elif report['risk_score'] > 40:\n",
    "        report['recommendations'].append(\"HIGH: Significant PII exposure detected\")\n",
    "        report['recommendations'].append(\"Implement data masking for sensitive fields\")\n",
    "        report['recommendations'].append(\"Enable audit logging for all data access\")\n",
    "    else:\n",
    "        report['recommendations'].append(\"MODERATE: Standard security measures recommended\")\n",
    "        report['recommendations'].append(\"Regular security reviews recommended\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate compliance report\n",
    "compliance_report = generate_compliance_report(results)\n",
    "\n",
    "print(\"COMPLIANCE REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Generated: {compliance_report['timestamp']}\")\n",
    "print(f\"\\nRisk Score: {compliance_report['risk_score']:.1f}/100\")\n",
    "print(f\"Total PII Entities: {compliance_report['total_pii_entities']}\")\n",
    "\n",
    "print(\"\\nAffected Regulations:\")\n",
    "for reg in compliance_report['affected_regulations']:\n",
    "    print(f\"\\n  📋 {reg['regulation']}\")\n",
    "    print(f\"     Affected PII Types: {', '.join(reg['affected_pii_types'])}\")\n",
    "    print(f\"     Requirements: {reg['requirements']}\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, rec in enumerate(compliance_report['recommendations'], 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "This notebook has demonstrated a comprehensive PII detection system using:\n",
    "\n",
    "1. **Named Entity Recognition (NER)** - Detecting person names, organizations, and locations\n",
    "2. **Regex Pattern Matching** - Finding structured PII like SSNs, emails, and phone numbers\n",
    "3. **Proximity Analysis** - Identifying high-risk PII combinations based on spatial relationships\n",
    "4. **Graph Theory** - Building entity relationship networks to find PII clusters\n",
    "5. **Risk Assessment** - Multi-tier classification of PII exposure risk\n",
    "6. **Compliance Reporting** - Automated regulatory compliance assessment\n",
    "\n",
    "The system provides enterprise-grade PII detection and protection capabilities suitable for production deployment in privacy-sensitive environments.\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ❤️ for data privacy and security**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}